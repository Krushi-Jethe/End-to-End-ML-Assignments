
# Code Structuring 

The aim of this assignment is to **learn how to structure and organize a Machine Learning repository** so that it's scalable, maintainable, and ready for team collaboration or production deployment.

By completing this assignment, you will:

- Organize your ML project into well-defined modules
- Learn to use `@dataclass`, `__dunder__` methods like `__str__`, `__call__`, `__getitem__`, etc.
- Understand the role of `__init__.py` and relative imports in Python packages
- Use `pylint` for code linting and `autodocstring` for clean and consistent docstrings

---

## üß∞ Key Concepts to Apply

### üîÅ Relative Imports

Use **relative imports** within your `src` directory to keep modules portable and reusable.

Example inside `main.py`:
```python
from src.data.processor import DataProcessor
from src.models.trainer import ModelTrainer
from src.utils.config import TrainConfig
```

Make sure every subfolder has an empty `__init__.py` file to treat it as a package. You can leave it empty or define package-level exports like this:
```python
# __init__.py inside data/
from .processor import DataProcessor
```

---

### üßë‚Äçüíª Recommended Class Design

You are expected to use object-oriented programming

#### ‚úÖ Use `@dataclass` in `utils/config.py`
```python
from dataclasses import dataclass

@dataclass
class TrainConfig:
    model_name: str
    data_path: str
    model_path: str
    target_column: str
    test_size: float = 0.2
```

#### ‚úÖ Use `__dunder__` methods where appropriate
| Class | Method | Description |
|-------|--------|-------------|
| `DataProcessor` | `__getitem__`, `__str__` | Quick column access, better debug prints |
| `ModelTrainer` | `__call__`, `__str__`, `__len__` | Callable for prediction, length for sample size |
| `InferenceEngine` | `__call__` | Callable for inference with thresholds |

---

### üß™ Running the Pipeline

Your `main.py` should accept arguments using `argparse`:
```bash
python src/main.py   --data_path data/raw/xyz.csv   --target target_column   --model lgbm   --model_path models/model.pkl   --infer
```

---

## üßπ Code Quality Standards

### üßë‚Äçüîß 1. PEP-8 Compliance
Use the `black` formatter for consistent indentation, spacing, and style.

### üìé 2. Linting
Run `pylint` on your code and **resolve all errors**:
```bash
pylint src/ 
```

You should aim for a score > 8.5.

### üìù 3. Auto-generated Docstrings
Use the **"autoDocstring - Python Docstring Generator" extension** by Nils Werner in VSCode to automatically generate docstrings for:

- All modules & submodules
- All classes
- All functions and methods

Sample:
```python
def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
    """
    Cleans raw input data by handling missing values and formatting.

    Args:
        df (pd.DataFrame): Raw input DataFrame.

    Returns:
        pd.DataFrame: Cleaned DataFrame.
    """
```

---

## üìÇ Deliverables

Create or update the following files based on functions created in previous notebooks:
- `src/data/processor.py` with `DataProcessor` class
- `src/models/trainer.py` with `ModelTrainer` class
- `src/models/inference.py` with `InferenceEngine`
- `src/utils/config.py` with `TrainConfig`, `InferenceConfig`
- `src/main.py` to run your training pipeline via argparse

---

## ‚úÖ Checklist

- [ ] Update `__init__.py` in all submodules  
- [ ] Define `DataProcessor`, `ModelTrainer`, and `InferenceEngine` classes  
- [ ] Use `@dataclass` for configuration  
- [ ] Use relevant `__dunder__` methods  
- [ ] Add docstrings 
- [ ] Ensure code passes `pylint` without major issues  
- [ ] Use `argparse` in `main.py`
- [ ] Update setup.py

---

## üèÅ Optional

- [ ] Add logging instead of `print()`  
- [ ] Add unit tests with `pytest` in test directory
- [ ] Move pipeline into a `PipelineRunner` class  

