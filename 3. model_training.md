
# Model Training & Hyperparameter Tuning

The aim of this assignment is to **familiarize yourself with the complete pipeline of model training, evaluation, and hyperparameter tuning**. By the end of this exercise, you should have a good understanding of how to take a clean, feature-rich dataset and build optimized models with it.

---

### üõ†Ô∏è Exploration of Scikit-learn & Model Evaluation Tools

*Note: These are commonly used tools and functions. You are encouraged to read more and experiment further to build deeper understanding.*

#### üß∞ Model Selection, Training & Evaluation:
- `train_test_split` from `sklearn.model_selection`
- `Pipeline`, `ColumnTransformer`
- `StandardScaler`, `OneHotEncoder`
- `RandomForestClassifier`, `LogisticRegression`, `GradientBoostingRegressor`, etc.
- `fit`, `predict`, `predict_proba`
- `classification_report`, `confusion_matrix`, `roc_auc_score`, `roc_curve`, `f1_score`
- `cross_val_score`, `StratifiedKFold`, `KFold`
- `GridSearchCV`, `RandomizedSearchCV`
- `make_pipeline`, `make_column_selector`, `make_column_transformer`

#### üìà Visualizations (Optional but helpful):
- ROC Curve, Precision-Recall Curve
- Feature importance plots (especially for tree-based models)
- Confusion matrix heatmap using seaborn

---

### üß™ Experiment Tracking & Reproducibility
- Save your best model using `joblib` or `dill`
- Save evaluation metrics as a dictionary (you can write it to a `.json`)
- Track hyperparameters and results in a `.csv`

---

### üìö Assignment Instructions

As with previous assignments, writing **clean, modular, and reusable code** is expected. Follow **PEP-8 guidelines**, and format notebooks using `%load_ext jupyter_black`. Use meaningful naming conventions.

---

### üìÅ Notebook Structure

Create **two Jupyter notebooks** in your `notebooks` directory, naming them as per the standard format:  
`[notebook-number]-[initials]-[title].ipynb` (e.g., `02-kj-model-training.ipynb`)

---

## üìù Notebook 1: Model Training & Evaluation

Structure your notebook like this:

```
# 1. Imports
%load_ext jupyter_black

# 2. Data Loading

# 3. Data Preparation (read the cleaned data from the previous assignment or notebook)

# 4. Model Training & Evaluation

## 4.1 Baseline Model
### 4.1.1 Train/Test Split
### 4.1.2 Train baseline model (e.g., Logistic Regression, Random Forest)
### 4.1.3 Evaluate model (accuracy, F1, ROC-AUC, confusion matrix, etc.)

## 4.2 Cross Validation

## 4.3 Train and Evaluate Multiple Models

# 5. Save metrics, confusion matrix, and best model

# 6. Conclusion
```

- [ ] Load processed dataset from `data/processed`
- [ ] Split into train/test/validation (if not already done)
- [ ] Try out at least **1 model** from Scikit-learn, **1 model** which is either (LGBM or XGBoost) and another model of your choice.
- [ ] Evaluate them using proper metrics and tabulate the results
- [ ] Save model using `joblib` or `dill`
- [ ] Save evaluation results to JSON
- [ ] End with a summary and observations

---

## üß™ Notebook 2: Hyperparameter Tuning

```
# 1. Imports

# 2. Load data

# 3. Create pipeline (include preprocessing + modeling)

# 4. Hyperparameter Tuning

## 4.1 GridSearchCV or RandomizedSearchCV
## 4.2 Best model and its parameters
## 4.3 Evaluate best model on test data

# 5. Save best estimator and tuned results

# 6. Conclusion
```

- [ ] Use either `GridSearchCV` or `RandomizedSearchCV`
- [ ] Tune at least 3 hyperparameters of one selected model
- [ ] Save the best model and hyperparameter details
- [ ] Evaluate on test set
- [ ] Compare performance with your baseline model
- [ ] Summarize your learnings

---

### ‚úÖ Checklist Before Submission

- [ ] Follow clean code practices and naming conventions
- [ ] Include markdown cells with explanations
- [ ] Plot confusion matrix or ROC curves where applicable
- [ ] Save models and results in `models/` and `results/` folders respectively
- [ ] Final results and observations clearly written in markdown
