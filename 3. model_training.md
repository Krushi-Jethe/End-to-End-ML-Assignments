
# Model Training & Hyperparameter Tuning

The aim of this assignment is to **familiarize yourself with the complete pipeline of model training, evaluation, and hyperparameter tuning**. By the end of this exercise, you should have a good understanding of how to take a clean, feature-rich dataset and build optimized models with it.

---

### üõ†Ô∏è Exploration of Scikit-learn & Model Evaluation Tools

*Note: These are commonly used tools and functions. You are encouraged to read more and experiment further to build deeper understanding.*

#### üß∞ Model Selection, Training & Evaluation:
- `Pipeline`, `ColumnTransformer`
- `StandardScaler`, `OneHotEncoder`
- `RandomForestClassifier`, `LogisticRegression`, `GradientBoostingRegressor`, etc.
- `fit`, `predict`, `predict_proba`
- `classification_report`, `confusion_matrix`, `roc_auc_score`, `roc_curve`, `f1_score`
- `cross_val_score`, `StratifiedKFold`, `KFold`
- `GridSearchCV`, `RandomizedSearchCV`
- `Optuna` Package for Hyperparameter tuning

#### üìà Visualizations:
- ROC Curve, Precision-Recall Curve
- Feature importance plots (for tree-based models)
- Confusion matrix heatmap using seaborn

---

### üß™ Experiment Tracking & Reproducibility
- Save your best pipeline using `joblib` or `dill`
- Track hyperparameters, results using a dictionary (you can write it to a `.json`) as given below

_Given below is just an example feel free to modify as you see fit_
```python
{
    "num_of_samples_used": "<int - Number of samples used for model training>",
    "features_used": [
        "<list of feature names used in model training>" # You can pass X.columns as list here
    ],
    "model": "<str - Name of the model used (e.g., 'XGBClassifier')>",
    "iter_num": "<int - Iteration or trial number of tuning or experiment>",
    
    "model_param": {
        "n_estimators": "<int - Number of boosting rounds>",
        "max_depth": "<int - Maximum tree depth>",
        "learning_rate": "<float - Learning rate/shrinkage factor>",
        "subsample": "<float - Fraction of samples to be used for fitting each base learner>",
        "colsample_bytree": "<float - Fraction of features to be used per tree>",
        "scale_pos_weight": "<float - Weight balancing for imbalanced classes>"
    },
    
    "model_training_time": "<str - Time taken to train model in HH:MM:SS.microseconds format>",
    
    "model_metrics": {
        "accuracy": "<float - Overall accuracy of the model>",
        "precision": "<float - Precision score>",
        "recall": "<float - Recall score>",
        "f1_score": "<float - F1 score>",
        "roc_auc": "<float - Area under ROC curve>",
        "pr_auc": "<float - Area under Precision-Recall curve>",
        
        "confusion_matrix": {
            "TP": "<int - True Positives>",
            "FP": "<int - False Positives>",
            "FN": "<int - False Negatives>",
            "TN": "<int - True Negatives>"
        }
    },    
    "grid_search_param": {
        "num_trials": "<int - Total number of optimization trials>",
    
        "n_estimators": {
            "min": "<int - minimum value>",
            "max": "<int - maximum value>"
        },
    
        "max_depth": {
            "min": "<int - minimum value>",
            "max": "<int - maximum value>"
        },
    
        "learning_rate": {
            "min": "<float - minimum value>",
            "max": "<float - maximum value>"
        },
    
        "subsample": {
            "min": "<float - minimum value>",
            "max": "<float - maximum value>"
        },
    
        "colsample_bytree": {
            "min": "<float - minimum value>",
            "max": "<float - maximum value>"
        },
    
        "scale_pos_weight": {
            "min": "<float - minimum value>",
            "max": "<float - maximum value>"
        }
    }
}
```  

---

### üìö Assignment Instructions

As with previous assignments, writing **clean, modular, and reusable code** is expected. Follow **PEP-8 guidelines**, and format notebooks using `%load_ext jupyter_black`. Use meaningful naming conventions.

---

### üìÅ Notebook Structure

Create **two Jupyter notebooks** in your `notebooks` directory, naming them as per the standard format:  
`[notebook-number]-[initials]-[title].ipynb` (e.g., `02-kj-model-training.ipynb`)

---

## üìù Notebook 1: Model Training & Evaluation

Structure your notebook like this:
*Structure of notebook can be modified based on your use case and convenience*

```
# 1. Imports
%load_ext jupyter_black

# 2. Data Loading

# 3. Data Preparation (read the cleaned data from the previous assignment or notebook)

# 4. Model Training & Evaluation

## 4.1 Baseline Model
### 4.1.1 Load Train/Val/Test Split
### 4.1.2 Train baseline model (e.g., Logistic Regression, Random Forest)
### 4.1.3 Evaluate model (accuracy, F1, ROC-AUC, confusion matrix, etc.)

## 4.2 Cross Validation

## 4.3 Train and Evaluate Multiple Models

# 5. Save metrics, confusion matrix, and best model

# 6. Conclusion
```

- [ ] Load processed & cleaned dataset from `data/processed`
- [ ] Try out at least **1 model** from Scikit-learn, **1 model** which is either (LGBM or XGBoost) and another model of your choice.
- [ ] Evaluate them using proper metrics and tabulate the results
- [ ] End with a summary and observations
- [ ] You are supposed to track each and every experiment and save relevant info.
  - [ ] Save experimentation data i.e., various metrics, model info, time taken etc. in `data/experiments` directory. 
  - [ ] Save trained models to `models` directory using `joblib` or `dill`

---

## üß™ Notebook 2: Hyperparameter Tuning

Structure your notebook as given below:
*Structure of notebook can be modified based on your use case and convenience*
```
# 1. Imports

# 2. Load data

# 3. Create pipeline (include preprocessing + modeling)

# 4. Hyperparameter Tuning

## 4.1 GridSearchCV or RandomizedSearchCV or Optuna
## 4.2 Best model and its parameters
## 4.3 Evaluate best model on test data

# 5. Save best estimator and tuned results

# 6. Conclusion
```

- [ ] Use either `GridSearchCV` or `RandomizedSearchCV` or `Optuna`
- [ ] Tune at least 3 hyperparameters of one selected model
- [ ] Save the best model and hyperparameter details
- [ ] Evaluate on test set
- [ ] Compare performance with your baseline model
- [ ] Summarize your learnings

---

